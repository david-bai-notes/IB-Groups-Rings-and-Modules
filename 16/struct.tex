\section{The Structure Theorem}
Until further notice, we take our ring to be a ED and we denote its Euclidean function by $\phi:R^\times\to\mathbb Z_{\ge 0}$.
let $A$ be an $m\times n$ matrix with entries in $R$.
\begin{definition}
    The elementary row operations are as follows:\\
    1. Add $\lambda\in R$ times the $j^{th}$ row to the $i^{th}$ row for $i\neq j$.\\
    2. Swap the $i^{th}$ and $j^{th}$ row.\\
    3. Multiply the $i^{th}$ row by a unit $u$.
\end{definition}
Note that all these operations are reversible.
Also, each of the operations may be realized by multiplying in the left by an $m\times m$ invertible matrix.
To wit, the first operation is the left multiplication of the matrix $I+\lambda E_{ij}$ where $E_{ij}$ is the matrix with $1$ on the $(i,j)$ entry and $0$ otherwise.
The second is $I+E_{ij}+E_{ji}-E_{ii}-E_{jj}$ and the third is $I+(u-1)E_{ii}$.
We can similarly define the column operations, and the realization becomes the matrix multiplication on the right by an $n\times n$ invertible matrix analogous to before.
\begin{definition}
    Two $m\times n$ matrices $A,B$ are equivalent if there is a sequence of elementary row and column operations taking $A$ to $B$.
\end{definition}
It is obvious that this is an equivalence relation due to reversibility.
So if $A,B$ are equivalent, there exists invertible square matrices $P,Q$ with $B=QAP$.
\begin{theorem}[Smith Normal Form]
    An $m\times n$ matrix $A=(a_{ij})$ with entries in $R$ is equivalent to a diagonal matrix of the form
    $$\begin{pmatrix}
        d_1&&&&&&\\
        &d_2&&&&&\\
        &&\ddots&&&&\\
        &&&d_t&&&\\
        &&&&0&&\\
        &&&&&\ddots&\\
        &&&&&&0
    \end{pmatrix}$$
    where $d_i\neq 0$ for all $i$ and $d_1|d_2|\cdots |d_t$.
\end{theorem}
The $d_i$'s are called covariant factors and are unique up to associates (show later).
\begin{proof}
    If $A=0$ then we are done.
    So assume that $a_{11}\neq 0$.
    Suppose $a_{11}\nmid a_{1j}$ for some $j\ge 2$, then we use the Euclidean algorithm to get $a_{1j}=qa_{11}+r$ with $q,r\in R$ and $\phi(r)<\phi(a_{11})$.
    So we substract $q$ times the first column from the $j^{th}$ column and swap them.
    This makes the top left entry $r$.
    Likewise, we can do the same thing if $a_{11}\nmid a_{i1}$ for some $i\ge 2$ by row operations.
    The cases above each decrease the top left entry (in terms of its value in the Euclidean function), thus it must eventually stops, at which time $a_{11}$ divides everything in the first row or the first column.
    Then substracting multiples of the first row or first column can clear up every other entry in the first column or first row except $a_{11}$.
    Now if $a_{11}$ does not divide $a_{ij}$ for $i,j\ge 2$, then add the $i^{th}$ row to the first row, and so we can do the same column operations to decrease $\phi(a_{11})$ further till it terminates.
    Do this over and over again (but finitely many times) we can get $a_{11}|a_{ij}$ for any $i,j\ge 2$ and $a_{ij}=0$ whenever $i\neg j$ and one of $i,j$ is $1$.
    We rename $a_{11}$ as $d_1$ and repeat the process on the smaller matrix by removing the first row and first column to give the existence.
\end{proof}
\begin{definition}
    A $k\times k$ minor of a matrix $A$ is the determinant of a $k\times k$ submatrix of $A$.
\end{definition}
\begin{definition}
    For a matrix $A$ over a ring $R$, we define the $k^{th}$ Fitting ideal $\operatorname{Fit}_k(A)$ be the ideal generated by all the $k\times k$ minors of $A$.
\end{definition}
\begin{lemma}
    If $A,B$ are equivalent matrices, then $\operatorname{Fit}_k(A)=\operatorname{Fit}_k(B)$ for any $k$ (such that the Fitting ideal makes sense).
\end{lemma}
\begin{proof}
    Suffices to show that the elementary operations do not change the Fitting ideal.
    The second and third row operations are trivial.
    For the first kind of row operation, suppose we add $\lambda$ times the second row to the first row so that $A=(a_{ij})$ becomes
    $$A'=\begin{pmatrix}
        a_11+\lambda a_{21}&a_{12}+\lambda a_{22}&\dots&a_{1n}+\lambda a_{2n}\\
        a_{21}&a_{22}&\dots&a_{2n}\\
        \vdots&\vdots&\ddots&\vdots\\
    \end{pmatrix}$$
    Let $C$ be a $k\times k$ submatrix of $A$ and $C'$ be its correspondent submatrix in $A'$.
    If $C$ does not intersect the first row, then $\det C=\det C'$.
    If $C$ intersect both the first and second row, we also have $\det C=\det C'$ since its simply a row operation on the $k\times k$ submatrix.
    If $C$ intersects the first but not the second row, then by expanding along the first row, the determinant of $\det C'$ would be $\det C+\lambda\det D$ for some other $k\times k$ submatrix of $A$.
    So $\det C'\in \operatorname{Fit}_k(A)$, so $\operatorname{Fit}_k(A')\subset \operatorname{Fit}_k(A)$.
    Since row operations are reversible, we also have the reverse inclusion.
\end{proof}
\begin{proposition}
    The covarient factors are unique up to associates.
\end{proposition}
\begin{proof}
    Look at the Fitting ideals.
\end{proof}
\begin{example}
    Consider the following matrix (here $\to$ represents row/column operations)
    $$A=\begin{pmatrix}
        2&-1\\
        1&2
    \end{pmatrix}\to\begin{pmatrix}
        1&-1\\
        3&2
    \end{pmatrix}\to\begin{pmatrix}
        1&0\\
        3&5
    \end{pmatrix}\to\begin{pmatrix}
        1&0\\
        0&5
    \end{pmatrix}$$
    One can also get its Smith normal form by considering the minors.
    Indeed, $\operatorname{Fit}_1(A)=(1)$, so $d_1=\pm 1$.
    Also $\operatorname{Fit}_2(A)=(5)$, hence $d_2=\pm 5$.
    So we obtain the Smith normal form.
\end{example}
\begin{theorem}
    Let $R$ be an Euclidean domain and $N$ is an $R$-submodule of $R^n$, then there is a free basis $x_1,\ldots,x_m$ of $R^m$ such that $N$ is generated as an $R$-module by $d_1x_1,\ldots,d_tx_t$ for some $t\le m$ and $d_1|d_2|\cdots|d_t$.
\end{theorem}
\begin{proof}
    $R$ is a ED hence a PID, hence $N$ is generated by some $y_1,\ldots,y_n$ for some $n\le m$.
    Now each $y_i$ is in $R^n$, so we can form an $n\times n$ matrix $A$ whose columns are the $y_i$'s.
    By the preceding theorem, $A$ is equivalent to $A'=\operatorname{diag}(d_1,\ldots,d_t,0,\ldots,0)$ with $t\le n$ and $d_1|d_2|\ldots|d_t$.
    Now $A'$ is obtained from $A$ by elementary row and column operations.
    Each row operation corresponds to changing of our choice of free basis for $R^n$, and every column operation changes the generating set for $R^n$.
    So after changing the free basis for $R^n$ to, say, $x_1,\ldots, x_n$, then $N$ is generated by $d_1x_1,\ldots,d_tx_t$.
\end{proof}
\begin{theorem}[Structure Theorem]
    Let $R$ be a Euclidean Domain and $M$ a finitely-generated $R$-module, then $M\cong R/(d_1)\oplus\cdots\oplus R/(d_t)\oplus R\oplus\cdots\oplus R$ for some $d_i\in R$ and $d_1|d_2|\cdots|d_t$.
\end{theorem}
These $d_i$'s are called invariant factors.
\begin{proof}
    Since $M$ is finitely generated, then we can find a surjective $R$-module map $\phi:R^m\to M$ for some $m$.
    Then $M\cong R^m/\ker\phi$, but by the preceding theorem, there exists a free basis $x_1,\ldots,x_n$ for $R^m$ such that $N=\ker\phi=Rd_1x_1+\cdots Rd_tx_t$ for $d_1|d_2|\cdots|d_t$, hence
    $$M\cong\frac{R\oplus R\oplus\cdots\oplus R\oplus R\oplus\cdots\oplus R}{Rd_1\oplus Rd_2\oplus\cdots\oplus Rd_t\oplus 0\oplus\cdots\oplus 0}\cong R/(d_1)\oplus\cdots\oplus R/(d_t)\oplus R\oplus\cdots\oplus R$$
    which is what we wanted.
\end{proof}
\begin{definition}
    Let $M$ be an $R$-module.
    An element $m\in M$ is called torsion if $\exists r\in R\setminus\{0\},rm=0$.\\
    $M$ is called a torsion module if every $m\in M$ is torsion.
    $M$ is called torsion-free if the only torsion is $0$.
\end{definition}
\begin{corollary}
    Let $R$ be an Euclidean domain, then any finitely generated torsion-free $R$-module is free.
\end{corollary}
\begin{proof}
    By the preceding theorem $M\cong R/(d_1)\oplus\cdots\oplus R/(d_t)\oplus R\oplus\cdots\oplus R$, but as $M$ is torsion-free, it cannot contain any of $R/(d_i)$ as $R$-submodule as they would contain torsions.
    Hence $M\cong R\oplus\cdots\oplus R$, therefore $M$ is free.
\end{proof}
\begin{remark}
    The structure theorem in fact holds whenever $R$ is a PID.
    Also, there is also a uniqueness statement in the structure theorem: Suppose no $d_i$'s is a unit (otherwise they only contribute $0$ factors to the product), then the module $M$ uniquely determines $d_1,\ldots,d_t$.
\end{remark}
\begin{example}
    Consider an abelian groupn $G$ generated by $a,b$ subject to relations $2a+b=-a+2b=0$.
    So $G\cong\mathbb Z^2/N$ where $N$ is generated by $(2,1),(-1,2)$, so we take (as in the proof of the structure theorem)
    $$A=\begin{pmatrix}
        2&-1\\
        1&2
    \end{pmatrix}\to\begin{pmatrix}
        1&0\\
        0&5
    \end{pmatrix}$$
    as seen before.
    So we can change basis for $\mathbb Z^2$ to generate $N$ by $(1,0),(0,5)$, hence $G\cong\mathbb Z\oplus\mathbb Z/(\mathbb Z\oplus 5\mathbb Z)\cong\mathbb Z/5\mathbb Z$.
\end{example}
More generally, for finitely generated abelian groups, we have the following:
\begin{theorem}
    Any finitely generated abelian group $G$ is isomorphic to
    $$\mathbb Z/d_1\mathbb Z\oplus\cdots\oplus\mathbb Z/d_t\mathbb Z\oplus\mathbb Z^r$$
    where $r\ge 0$ and $d_1|d_2|\cdots|d_t$.
\end{theorem}
The $r$ here stands for the rank of the group.
\begin{proof}
    Take $R=\mathbb Z$ in the structure theorem.
\end{proof}
In the special case that $G$ is finite, we immediately obtain Theorem \ref{fin_abe_struct}.
\begin{remark}
    Let $A,B$ be square matrices over $R$, then $\det (AB)=\det A\det B$, also $\operatorname{adj}(A)A=A\operatorname{adj}(A)=\det(A)I$.
    In particular, $A$ is invertible iff $\det A$ is a unit.
\end{remark}
\begin{theorem}[Cayley-Hamilton]
    Let $A=(a_{ij})$ be a $n\times n$ matrix over a field $F$.
    Let $\chi_A(X)=\det (XI-A)\in F[X]$, then $\chi_A(A)=0$.
\end{theorem}
\begin{proof}
    Consider $V=F^n$ as a $F[X]$-module. with $X$ acting as $A$, i.e. $f(X)\cdot v=f(A)v$.
    Let $e_1,\ldots,e_n$ be the standard basis for $F^n$, so for any $j$, $X\cdot e_j=\sum_{i=1}^na_{ij}e_i$, hence
    $$(XI-A)\begin{pmatrix}
        e_1\\
        \vdots\\
        e_n
    \end{pmatrix}=0$$
    By multiplying both sides by the adjugate of $XI-A$, we know that for any $i$,
    $$\chi_A(X)\cdot e_i=\det(XI-A)\cdot e_i=0\implies\chi_A(A)e_i=0$$
    But this can only happen when $\chi_A(A)=0$.
\end{proof}
Recall that we have mentioned Theorem \ref{classify_fin_abe}, which is yet another way of classifying the finite abelian groups by writing it as a product of cyclic $p$-groups.
Indeed, we can achieve this by generalising our structure theorem one step further.
\begin{lemma}
    Let $R$ be a PID and $a,b\in R$ has $\gcd(a,b)=1$ (up to associates), then there is an isomorphism of $R$-modules
    $$R/(ab)\cong R/(a)\oplus R/(b)$$
\end{lemma}
\begin{proof}
    Since $R$ is a PID, $(a,b)=(d)$ for some $d\in R$, so $d=\gcd(a,b)$ by certain questions in example sheet.
    Hence $(a,b)=R$ since $d$ must be a unit by hypothesis, so there is $r,s\in R$ with $ra+sb=1$.
    Define an $R$-module homomorphism $\phi:R\to R/(a)\oplus R/(b)$ by $x\mapsto (x+(a),x+(b))$.
    To see it is surjective, $\phi(sb)=(1+(a),0+(b)),\phi(ra)=(0+(a),1+(b))$, hence $\phi(sbx+ray)=(x+(a),y+(b))$.
    Now if $\phi(x)=(0+(a),0+(b))$, then $x\in (a)\cap(b)$, so $x=x(ra+sb)=rax+sxb\in (ab)$.
    It is obvious that anything in $(ab)$ is mapped to zero, hence $\ker\phi=(ab)$, so we deduce the theorem from isomorphism theorem.
\end{proof}
This reduced to Chinese Remainder Theorem when we set $R=\mathbb Z$.
\begin{theorem}[Prime Decomposition Theorem]\label{prime_decomp}
    Let $R$ be a ED and let $M$ be a finitely generated $R$-module.
    Then
    $$M\cong R/(p_1^{n_1})\oplus\cdots\oplus R/(p_k^{n_k})\oplus R^m$$
    where $p_1,\ldots,p_k$ are prime in $R$.
\end{theorem}
Note that $p_1,\ldots,p_k$ need not to be distinct.
\begin{proof}
    By the structure theorem, we have
    $$M\cong R/(d_1)\oplus\cdots\oplus R/(d_t)\oplus R^m$$
    So it suffices to write each $R/(d_i)$ in the desired form.
    Choose $i$ and write $d_i=up_1^{\alpha_1}\cdots p_r^{\alpha_r}$ where $p_i$ are pairwise non-associates and $u$ is a unit.
    So by the preceding lemma, we have
    $$R/(d_i)\cong R/(p_1^{\alpha_1})\oplus\cdots\oplus R/(p_r^{\alpha_r})$$
    which establishes the theorem.
\end{proof}
Note that Theorem \ref{classify_fin_abe} is a direct consequence of this.\\
Let $V$ be a vector space over a field $F$ and let $\alpha:V\to V$ be an endomorphism, then we can make $V$ an $F[X]$-module (written as $V_\alpha$) by $f(X)\cdot v=f(\alpha)v$.
\begin{lemma}
    If $V$ is finite-dimensional, then $V_\alpha$ is finitely generated as an $F[X]$-module.
\end{lemma}
\begin{proof}
    If $v_1,\ldots,v_n$ generates $V$ as a vector space, then they also generate $V_\alpha$ as an $F[X]$-module since $F\le F[X]$.
\end{proof}
The lemma itself is trivial, but the thing to take from here is that $V_\alpha$, being also an $F$-vector space, is isomorphic to $V$.
This is very useful if we want to analyze the behaviour of $\alpha$ in $V$:
If we know that $V_\alpha$ is isomorphic to some $F[X]$-modules (via e.g. Theorem \ref{prime_decomp}) that are easier to study, then they are also automatically isomorphic as $F$-vector spaces.
So by choosing a nice basis for this $F[X]$-module (as a vector space), we can obtain a nice matrix of $\alpha$.
\begin{example}
    1. Suppose $V_\alpha\cong F[X]/(X^n)$ as $F[X]$-module, then we can choose the basis $1,X,\ldots,X^{n-1}$ for it to be an $F$-vector space, so $\alpha$ would have matrix
    $$(\star)=\begin{pmatrix}
        0&&&\\
        1&0&&\\
        &\ddots&\ddots&\\
        &&1&0
    \end{pmatrix}$$
    Since $\alpha$ acts as multiplication by $X$.\\
    2. Suppose $V_\alpha\cong F[X]/(X-\lambda)^n$ as an $F[X]$-module, then wrt the basis $1,X-\lambda,\ldots,(X-\lambda)^{n-1}$, $\alpha-\lambda\operatorname{id}$ has matrix $(\star)$, so $\alpha$ exists as a Jordan block.\\
    3. Suppose $V_\alpha\cong F[X]/(f)$ where $f\in F[X]$ is in the form $f(X)=a_0+a_1X+\cdots +a_{n-1}X^{n-1}+X^n$, then with respect to $1,X,\ldots,X^{n-1}$, $\alpha$ has the matrix
    $$C(f)=\begin{pmatrix}
        0&&&-a_0\\
        1&\ddots&&-a_1\\
        &\ddots&0&\vdots\\
        &&1&-a_{n-1}
    \end{pmatrix}$$
    which is called the companion matrix of $f$.
\end{example}
\begin{theorem}[Rational Canonical Form]
    Let $\alpha:V\to V$ be an endomorphism of a finite dimensional vector space $V$ over a field $F$, then
    $$V_\alpha\cong F[X]/(f_1)\oplus\cdots\oplus F[X]/(f_t)$$
    where $f_i\in F[X]$ are monic and $f_1|f_2|\cdots|f_t$.
    Moreover, with respect to a suitably chosen basis for $V$, $\alpha$ has matrix of the form
    $$\begin{pmatrix}
        C(f_1)&&&\\
        &C(f_2)&&\\
        &&\ddots&\\
        &&&C(f_t)
    \end{pmatrix}$$
\end{theorem}
\begin{proof}
    $V_\alpha$ is finitely generated as an $F[X]$-module and since $F[X]$ is a ED, we can apply the structure theorem to get
    $$V_\alpha\cong F[X]/(f_1)\oplus\cdots\oplus F[X]/(f_t)\oplus F[X]^m$$
    where $f_i\in F[X]$ are monic and $f_1|f_2|\cdots|f_t$.
    $m=0$ since $V$ is finite dimensional over $F$.
    The result is immediate.
\end{proof}
\begin{remark}
    1. If we start by an $n\times n$ matrix of $\alpha$, then this matrix must be similar to the above form.\\
    2. The minimal polynomial of $\alpha$ is $f_t$.\\
    3. The characteristic polynomial of $\alpha$ is $f_1\cdots f_t$ (up to associates).
    Hence the minimal polynomial divides the characteristic polynomial, so we immediately have Cayley-Hamilton.
\end{remark}
\begin{example}
    When $V$ is $2$-dimensional vector space over $F$, then one of the following cases happen
    $$V_\alpha\cong F[X]/(X-\lambda_1)\oplus F[X]/(X-\lambda_2),V_\alpha\cong F[X]/(f)$$
    where $(X-\lambda_1)(X-\lambda_2)$ or $f$ is the characteristic polynomial of $\alpha$.
\end{example}
\begin{corollary}
    Let $A,B\in\operatorname{GL}_2(F)$ that are not scalar matrices, then $A,B$ are conjugate iff they have the same characteristic polynomial.
\end{corollary}
\begin{proof}
    Immediate.
\end{proof}
\begin{lemma}
    Primes in $\mathbb C[X]$ are polynomials $X-\lambda$ where $\lambda$ is any complex number (up to associates).
\end{lemma}
\begin{proof}
    FTA.
\end{proof}
\begin{theorem}[Jordan Normal Form]
    Let $\alpha:V\to V$ be an endomorphism of a finite dimensional vector space over $\mathbb C$.
    Let $V_\alpha$ be $V$ as the $\mathbb C[X]$-module with $X$ acting as $\alpha$.
    Then there is an isomorphism of $\mathbb C[X]$-modules
    $$V_\alpha\cong\mathbb C[X]/(X-\lambda_1)^{n_1}\oplus\cdots\oplus\mathbb C[X]/(X-\lambda_t)^{n_t}$$
    where $\lambda_i\in\mathbb C$ are not necessarily distinct and $n_i\in\mathbb N$.
    In particular, there is a basis for $V$ such that $\alpha$ has the matrix
    $$\begin{pmatrix}
        J_{n_1}(\lambda_1)&&\\
        &\ddots&\\
        &&J_{n_t}(\lambda_t)
    \end{pmatrix},J_n(\lambda)=\begin{pmatrix}
        \lambda&&&\\
        1&\lambda&&\\
        &\ddots&\ddots&\\
        &&1&\lambda
    \end{pmatrix}$$
\end{theorem}
\begin{proof}
    We know that $\mathbb C[X]$ is a ED and $V_\alpha$ is finitely generated as $V$ is finite dimensional.
    By Prime Decomposition Theorem, noting that primes in $\mathbb C[X]$ are linear factors, and we cannot have any copy of $\mathbb C[X]$ as the dimension is finite.
    Then we already have the isomorphism.
    Now $J_n(\lambda)$ represents the multiplication by $X$ on $\mathbb C[X]/(X-\lambda)^n$ wrt the basis $1,X-\lambda,\ldots,(X-\lambda)^{n-1}$.
    This shows the theorem.
\end{proof}
\begin{remark}
    1. The theorem implies that any matrix with entries in $\mathbb C$ is similar to a matrix in the said form.\\
    2. The Jordan blocks are unique up to reordering.\\
    3. The minimal polynomial of $\alpha$ is $\prod_i(X-\lambda_i)^{c_i}$ where $c_i$ is the size of the largest block with eigenvalue $\lambda_i$.\\
    4. The characteristic polynomial of $\alpha$ is $\prod_i(X-\lambda_i)^{a_i}$ where $a_i$ is the sum of the sizes of the blocks with eigenvalue $\lambda_i$.\\
    5. The eigenspace of $\lambda_i$ has dimension equal to the number of blocks with eigenvalue $\lambda$.\\
    6. The uniqueness statement may be proved by considering the dimension of the generalized eigenspaces $\ker((\alpha-\lambda_iI)^n)$, $n=1,2,\ldots$.
\end{remark}
\begin{theorem}
    The structure theorem is true for any PIDs.
\end{theorem}
We will not prove this in the course, but we will illustrate the trick that is used for this extension.
\begin{theorem}\label{pid_tf_free}
    Let $R$ be a PID, then any finitely generated torsion-free $R$-module is free.
\end{theorem}
Note that for $R$ a ED, this is a corollary of the structure theorem.
\begin{lemma}
    Let $R$ be a PID and $M$ an $R$-module.
    Let $r_1,r_2\in R$ that are not both $0$.
    Let $d=\gcd(r_1,r_2)$.\\
    1. There is a matrix $A\in\operatorname{SL}_2(R)$ such that
    $$A\begin{pmatrix}
        r_1\\
        r_2
    \end{pmatrix}=\begin{pmatrix}
        d\\
        0
    \end{pmatrix}$$
    2. If $x_1,x_2\in M$, then there is $x_1',x_2'\in M$ such that $Rx_1+Rx_2=Rx_1'+Rx_2'$ and $r_1x_1+r_2x_2=dx_1'+0x_2'$.
\end{lemma}
\begin{proof}
    We know that $(r_1,r_2)=(d)$, so there is $\alpha,\beta\in R$ such that $\alpha r_1+\beta r_2=d$.
    Write $r_1=s_1d,r_2=s_2d$ with $s_1,s_2\in R$.
    We simply take the matrix
    $$A=\begin{pmatrix}
        \alpha&\beta\\
        -s_2&s_1
    \end{pmatrix}\in\operatorname{SL}_2(R)$$
    and it works for the first part.
    For the second part, we can take $x_1'=s_1x_1+s_2x_2,x_2'=-\beta x_1+\alpha x_2$.
\end{proof}
\begin{proof}[Proof of Theorem \ref{pid_tf_free}]
    Say $M=Rx_1+\cdots+Rx_n$ with $n$ minimal.
    If $x_1,\ldots,x_n$ are independent, then the module is free.
    Otherwise, there is $r_1,\ldots,r_n\in R$ such that $r_1x_1+\cdots+r_nx_n=0$ where WLOG $r_1\neq 0$.
    By the preceding lemma, we can choose $x_1',x_2'$ such that $Rx_1+Rx_2=Rx_1'+Rx_2'$, so $M=Rx_1'+Rx_2'+Rx_3+\cdots+Rx_n$ and $dx_1'+r_3x_3+\cdots+r_nx_n=0$ for $d=\gcd(r_1,r_2)\neq 0$.
    Continue the process to $3,\ldots,n$ to reduce the case to $rx_1=0$ for some $r\neq 0$, but $R$ is torsion-free, so $x_1=0$, contradicting the minimality of $n$.
\end{proof}

